{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78361656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터에 맞게 code1과 code2 그리고 유사성에 대한 pair을 읽어\n",
    "# code에 대한 토큰화 작업과 토큰에 대한 index 작업\n",
    "# 토큰에 대한 index 작업은 전체 code의 토큰에 대한 사전을 만든후 단어에 대한 index를 사용\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# # pandas를 사용 open 폴더에 있는 sample_train.csv 파일 로드\n",
    "train = pd.read_csv(\"open/sample_train.csv\")\n",
    "train.head()\n",
    "\n",
    "# # 읽어온 데이터를 code1, code2, similar로 분리\n",
    "train_code1 = [train[\"code1\"][idx] for idx in range(len(train))]\n",
    "train_code2 = [train[\"code2\"][idx] for idx in range(len(train))]\n",
    "train_similar = [train[\"similar\"][idx] for idx in range(len(train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc9ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 읽어온 code를 학습용과 검증용으로 분리(8:2)\n",
    "# 학습용 code에 대한 단어별로 토큰화 후 딕셔너리 생성\n",
    "# 딕셔너리는 학습용 code의 단어에 대한 index용\n",
    "# 단어에 대한 index를 RNN 학습을 위한 텐서를 위해 사용\n",
    "import json\n",
    "\n",
    "# 사전을 파일에 저장 및 로드\n",
    "def save_dict(saveDict, fname):\n",
    "#     filename = 'dict.txt'\n",
    "    filename = fname\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps(saveDict))\n",
    "\n",
    "def load_dict(filename):\n",
    "    with open(filename) as f:\n",
    "        loadDict = json.loads(f.read())\n",
    "    return loadDict\n",
    "\n",
    "\n",
    "#code에 대한 토큰화 작업: 학습용 tcode, 검증용 vcode\n",
    "tcode1 = []\n",
    "tcode2 = []\n",
    "tsimilar = []\n",
    "\n",
    "vcode1 = []\n",
    "vcode2 = []\n",
    "vsimilar = []\n",
    "\n",
    "codedict = {\"unknown\": 0, \"Python padding\": 1} # 사전 정의후 새로운 단어에 대해서는 모두 unknown으로 처리\n",
    "\n",
    "# 읽어온 code에 대해 8:2로 분리후\n",
    "# 학습용 code에 대해서는 딕셔너리를 생성\n",
    "# 검증용 code에 대해서는 내용을 알 수 없기에 딕셔너리에 반영 안 함\n",
    "for idx in range(len(train)):\n",
    "    if idx%5 == 0:\n",
    "        vcode1.append(train_code1[idx].split())\n",
    "        vcode2.append(train_code2[idx].split())\n",
    "        vsimilar.append(train_similar[idx])\n",
    "    else:\n",
    "        tcode1.append(train_code1[idx].split())\n",
    "    #     dict1 = {key: i for i, key in enumerate(train[\"code1\"][idx].split())}\n",
    "        for i, key in enumerate(train_code1[idx].split()):\n",
    "            if key not in codedict:\n",
    "                codedict[key] = len(codedict)\n",
    "        tcode2.append(train_code2[idx].split())\n",
    "    #     dict1 = {key: i for i, key in enumerate(train[\"code1\"][idx].split())}\n",
    "        for i, key in enumerate(train_code1[idx].split()):\n",
    "            if key not in codedict:\n",
    "                codedict[key] = len(codedict)\n",
    "        tsimilar.append(train_similar[idx])\n",
    "\n",
    "# code를 분석한 딕셔너리에 대해 검증용 code를 위해 \"dict.txt\"로 저장\n",
    "save_dict(codedict, \"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 사전파일을 사용하여 코드에 대한 index 작업\n",
    "# 학습용 코드 index는 tidxcode, 검증용 코드 index는 vidxcode\n",
    "# 저장된 사전파일에서 로드\n",
    "idxdict = load_dict(\"dict.txt\")\n",
    "\n",
    "# RNN 학습에 사용할 Lookup Table에 대한 딕셔너리 단어갯수\n",
    "n_vocabs = len(idxdict)\n",
    "\n",
    "# 로드된 딕셔너리를 통해 토큰화된 학습용 code의 index 작업\n",
    "tidxcode1 = []\n",
    "tidxcode2 = []\n",
    "\n",
    "for idx in range(len(tcode1)):\n",
    "    idxcode1 = []\n",
    "    for icode in tcode1[idx]:\n",
    "        try:\n",
    "            idxcode1.append(idxdict[icode])\n",
    "        except:\n",
    "            idxcode1.append(0)\n",
    "    tidxcode1.append(idxcode1)\n",
    "    \n",
    "    idxcode2 = []\n",
    "    for icode in tcode2[idx]:\n",
    "        try:\n",
    "            idxcode2.append(idxdict[icode])\n",
    "        except:\n",
    "            idxcode2.append(0)\n",
    "    tidxcode2.append(idxcode2)\n",
    "\n",
    "# 로드된 사전을 통해 토큰화된 검증용 code의 index 작업\n",
    "vidxcode1 = []\n",
    "vidxcode2 = []\n",
    "\n",
    "for idx in range(len(vcode1)):\n",
    "    idxcode1 = []\n",
    "    for icode in vcode1[idx]:\n",
    "        try:\n",
    "            idxcode1.append(idxdict[icode])\n",
    "        except:\n",
    "            idxcode1.append(0)\n",
    "    vidxcode1.append(idxcode1)\n",
    "    \n",
    "    idxcode2 = []\n",
    "    for icode in vcode2[idx]:\n",
    "        try:\n",
    "            idxcode2.append(idxdict[icode])\n",
    "        except:\n",
    "            idxcode2.append(0)\n",
    "    vidxcode2.append(idxcode2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6041881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. 데이터셋 클래스 정의\n",
    "\n",
    "class NSMC_Dataset(Dataset):\n",
    "  def __init__(self, code1, code2, labels):\n",
    "    self.code1 = code1\n",
    "    self.code2 = code2\n",
    "    self.labels = labels\n",
    "    self.max_seq_len = 200\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # idx 번째 데이터 전처리\n",
    "    # code1, code2 ,label\n",
    "    labels = self.labels[idx]\n",
    "\n",
    "    code1 = self.code1[idx]\n",
    "    code2 = self.code2[idx]\n",
    "\n",
    "    # 정해진 길이로 맞추기\n",
    "    # max_seq_len보다 길면 코드의 뒷 부분 제거\n",
    "    # max_seq_len보다 짧으면 앞에 padding 추가 (Padding의 id는 토크나이저마다 다르지만, 여기서 사용한 토크나이저는 1을 사용)\n",
    "    if len(code1) > self.max_seq_len:\n",
    "      code1 = code1[:self.max_seq_len]\n",
    "    elif len(code1) < self.max_seq_len:\n",
    "      code1 = [1] * (self.max_seq_len-len(code1)) + code1\n",
    "\n",
    "    if len(code2) > self.max_seq_len:\n",
    "      code2 = code2[:self.max_seq_len]\n",
    "    elif len(code2) < self.max_seq_len:\n",
    "      code2 = [1] * (self.max_seq_len-len(code2)) + code2\n",
    "\n",
    "\n",
    "    return torch.LongTensor(code1), torch.LongTensor(code2), torch.FloatTensor([labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8075eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 클래스를 사용 data set을 만들어 학습에 사용할 data loader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 학습용 데이터 로더\n",
    "train_dataset = NSMC_Dataset(tidxcode1, tidxcode2, tsimilar)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "\n",
    "# 검증용 데이터 로더\n",
    "val_dataset = NSMC_Dataset(vidxcode1, vidxcode2, vsimilar)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d806b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 실행할 device 정의\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f10ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.RNN 모델 정의\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN_Text(nn.Module): \n",
    "  def __init__(self, n_vocabs):\n",
    "    super(RNN_Text, self).__init__()\n",
    "\n",
    "    # LookUp Table 정의\n",
    "    # 각 id에 해당하는 token들을 100차원 embedding으로 변환\n",
    "    # 모델과 같이 학습\n",
    "    self.embs = nn.Embedding(n_vocabs, 100)\n",
    "\n",
    "    self.hidden_dim = 32\n",
    "    self.n_layers = 1\n",
    "\n",
    "    # Pytorch에서 제공하는 RNN 모델\n",
    "    self.rnn = nn.RNN(100, self.hidden_dim, self.n_layers, batch_first=True)\n",
    "    \n",
    "    # RNN의 output을 100차원 embedding으로 변환\n",
    "    self.fc = nn.Linear(32, 100)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    x = self.embs(x)\n",
    "\n",
    "    hidden = self.init_hidden(batch_size)\n",
    "\n",
    "    # out: Batch Size X Max Seq Length X Hidden dim\n",
    "    out, hidden = self.rnn(x, hidden) \n",
    "    # out: Batch Size X Hidden dim (시퀀스의 마지막 embedding만 사용(문장 전체의 의미를 가지고 있기 때문))\n",
    "    out = self.fc(out[:,-1])\n",
    "    \n",
    "    return out, hidden\n",
    "  \n",
    "  # hidden_0을 만드는 함수\n",
    "  def init_hidden(self, batch_size):\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac36121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 클래스 인스턴스 생성과 로스기준 및 오티마이저 인스턴스 생성\n",
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = RNN_Text(n_vocabs*4)\n",
    "rnn_model = rnn_model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d1efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.055553656390143764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:10<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 0.05166832056310442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:11<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: 0.05711270841459433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:06<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 0.053258578293025494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:06<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: 0.05689590725633833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:06<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: 0.0508785424919592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:06<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: 0.04821241598162386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:06<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8: 0.04728371907439497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:05<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: 0.047342311247355405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:07<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.05532569466779629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:09<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: 0.04985505807523926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12: 0.046606764124913345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13: 0.04603374299282829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14: 0.04558596888143155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15: 0.04550135807030731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 0.05435775517589516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17: 0.0458631135068006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:09<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18: 0.04489193956471152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:08<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19: 0.04514520647625128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [01:09<00:00,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: 0.044676762552311025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델 학습\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-4)\n",
    "    \n",
    "for epoch_i in range(20):  # 에포크 반복\n",
    "  avg_loss = 0.0\n",
    "  # for batch in train_loader:\n",
    "  rnn_model.train()\n",
    "\n",
    "  for batch in tqdm(train_loader): # DataLoader의 code1에 대한 배치\n",
    "\n",
    "    inputs1, inputs2, labels = batch[0], batch[1], batch[2]\n",
    "    inputs1 = inputs1.to(device)\n",
    "    inputs2 = inputs2.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    rnn_optimizer.zero_grad() # 기울기 초기화\n",
    "    \n",
    "    outputs1, _ = rnn_model(inputs1) # 모델 forward\n",
    "    outputs2, _ = rnn_model(inputs2) # 모델 forward\n",
    "    \n",
    "    \n",
    "# # cosine 유사도 계산에서 극단적인 값이 나올 수 있으므로 정규화\n",
    "    norm_output1 = torch.linalg.norm(outputs1, dim=1)\n",
    "    norm_output2 = torch.linalg.norm(outputs2, dim=1)\n",
    "\n",
    "    outputs1 = outputs1 / norm_output1.unsqueeze(1)\n",
    "    outputs2 = outputs2 / norm_output2.unsqueeze(1)\n",
    "\n",
    "    # cosine 유사도 구하기\n",
    "    out = cos(outputs1, outputs2)\n",
    "\n",
    "    out = (out + 1) / 2.1 # 범위를 -1 ~ 1 => 0 ~ 1 로 변경\n",
    "\n",
    "    loss = criterion(out, labels.flatten()) # loss 계산\n",
    "\n",
    "    avg_loss += loss.item() # loss 값만 누적\n",
    "\n",
    "    loss.backward() # loss를 통해서 기울기 계산\n",
    "\n",
    "    rnn_optimizer.step() # 모델 업데이트\n",
    "    \n",
    "\n",
    "  print(f'{epoch_i+1}: {avg_loss/len(train_loader)}')\n",
    "\n",
    "torch.save(rnn_model.state_dict(), 'DictRNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bff125f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:34<00:00,  2.10it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 68.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.6662318835655848, Val accuracy 0.684195876121521\n",
      "\n",
      "Train epoch 2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [04:04<00:00,  1.84it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 71.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.38291754136482875, Val accuracy 0.9488035440444946\n",
      "\n",
      "Train epoch 3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:27<00:00,  2.17it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 82.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1469697206798527, Val accuracy 0.9755147695541382\n",
      "\n",
      "Train epoch 4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:06<00:00,  2.41it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 71.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.09949732880211538, Val accuracy 0.9805231094360352\n",
      "\n",
      "Train epoch 5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:15<00:00,  2.31it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 77.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.08608415928979715, Val accuracy 0.9788536429405212\n",
      "\n",
      "Train epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:12<00:00,  2.33it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 78.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.07902147372977601, Val accuracy 0.9757930040359497\n",
      "\n",
      "Train epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:10<00:00,  2.37it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 83.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.07659530447175106, Val accuracy 0.9855314493179321\n",
      "\n",
      "Train epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:32<00:00,  2.12it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 66.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.06698160926500957, Val accuracy 0.9849749803543091\n",
      "\n",
      "Train epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:30<00:00,  2.13it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 68.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.06544785767379735, Val accuracy 0.987200915813446\n",
      "\n",
      "Train epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 450/450 [03:05<00:00,  2.42it/s]\n",
      "100%|█████████████████████████████████████████| 113/113 [00:01<00:00, 92.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.06700875616735882, Val accuracy 0.9849749803543091\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 3. 모델 학습 및 평가\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# cosine similarity 계산을 위한 객체 생성\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-4)\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for epoch_i in range(10):\n",
    "  print(f'Train epoch {epoch_i+1}: ')\n",
    "  avg_loss = 0.0\n",
    "\n",
    "# 모델 학습\n",
    "  rnn_model.train()\n",
    "\n",
    "  # 학습 데이터로 학습\n",
    "  for batch in tqdm(train_loader):\n",
    "    code1_ids, code2_ids, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "    rnn_optimizer.zero_grad() # 기울기 초기화\n",
    "\n",
    "    outputs1, _ = rnn_model(code1_ids) # code1 임베딩\n",
    "    outputs2, _ = rnn_model(code2_ids) # code2 임베딩\n",
    "    \n",
    "    # cosine 유사도 계산에서 극단적인 값이 나올 수 있으므로 정규화\n",
    "    norm_output1 = torch.linalg.norm(outputs1, dim=1)\n",
    "    norm_output2 = torch.linalg.norm(outputs2, dim=1)\n",
    "\n",
    "    outputs1 = outputs1 / norm_output1.unsqueeze(1)\n",
    "    outputs2 = outputs2 / norm_output2.unsqueeze(1)\n",
    "\n",
    "    # cosine 유사도 구하기\n",
    "    out = cos(outputs1, outputs2)\n",
    "\n",
    "    # 유사도를 0 ~ 1 범위로 정규화\n",
    "    out = (out + 1) / 2.1 # 범위를 -1 ~ 1 => 0 ~ 1 로 변경\n",
    "\n",
    "    loss = criterion(out, labels.flatten()) # loss 계산\n",
    "    \n",
    "    avg_loss += loss.item() # loss 값만 누적\n",
    "\n",
    "    loss.backward() # loss를 통해서 기울기 계산\n",
    "\n",
    "    rnn_optimizer.step() # 모델 업데이트\n",
    "\n",
    "#   print(f'loss {avg_loss/len(train_loader)}')\n",
    "\n",
    "# 모델 검증\n",
    "  rnn_model.eval()\n",
    "\n",
    "  correct = 0\n",
    "\n",
    "  # 검증 데이터로 평가\n",
    "  for batch in tqdm(val_loader):\n",
    "    code1_ids, code2_ids, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs1, _ = rnn_model(code1_ids) # code1 임베딩\n",
    "      outputs2, _ = rnn_model(code2_ids) # code2 임베딩\n",
    "      \n",
    "      pred = cos(outputs1, outputs2)\n",
    "      pred = (pred + 1) / 2\n",
    "      \n",
    "      pred[pred > 0.5] = 1.0\n",
    "      pred[pred <= 0.5] = 0.0\n",
    "\n",
    "      correct += torch.sum(pred == labels.flatten())\n",
    "\n",
    "#   print(f'Val accuracy {correct/len(val_dataset)}')\n",
    "  print(f'loss {avg_loss/len(train_loader)}, Val accuracy {correct/len(val_dataset)}')\n",
    "  print()\n",
    "\n",
    "torch.save(rnn_model.state_dict(), 'DictRNN')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf7534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data tokenizing...\n",
      "Define class of Dataset & RNN_Text\n",
      "Starting validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5616/5616 [00:54<00:00, 102.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print('Reading data...')\n",
    "# 학습된 모델로 code 유사성 판단\n",
    "submission = pd.read_csv('open/test.csv')\n",
    "submission.head()\n",
    "\n",
    "# 읽어온 데이터를 code1, code2, similar로 분리\n",
    "test_code1 = [submission[\"code1\"][idx] for idx in range(len(submission))]\n",
    "test_code2 = [submission[\"code2\"][idx] for idx in range(len(submission))]\n",
    "test_pairs = [submission[\"pair_id\"][idx] for idx in range(len(submission))]\n",
    "\n",
    "fcode1 = []\n",
    "fcode2 = []\n",
    "for idx in range(len(submission)):\n",
    "    fcode1.append(test_code1[idx].split())\n",
    "    fcode2.append(test_code2[idx].split())\n",
    "\n",
    "print('Data tokenizing...')\n",
    "# code에 대한 index\n",
    "def load_dict(filename):\n",
    "    with open(filename) as f:\n",
    "        loadDict = json.loads(f.read())\n",
    "    return loadDict\n",
    "\n",
    "# 저장된 사전파일에서 로드\n",
    "idxdict = load_dict(\"dict.txt\")\n",
    "\n",
    "# 로드된 사전을 통해 토큰화된 유사성 판단용 code의 index 작업\n",
    "fidxcode1 = []\n",
    "fidxcode2 = []\n",
    "\n",
    "for idx in range(len(test_code1)):\n",
    "    idxcode1 = []\n",
    "    for icode in fcode1[idx]:\n",
    "        try:\n",
    "            idxcode1.append(idxdict[icode])\n",
    "        except:\n",
    "            idxcode1.append(0)\n",
    "    fidxcode1.append(idxcode1)\n",
    "    \n",
    "    idxcode2 = []\n",
    "    for icode in fcode2[idx]:\n",
    "        try:\n",
    "            idxcode2.append(idxdict[icode])\n",
    "        except:\n",
    "            idxcode2.append(0)\n",
    "    fidxcode2.append(idxcode2)\n",
    "\n",
    "print('Define class of Dataset & RNN_Text')\n",
    "    \n",
    "class FIN_Dataset(Dataset):\n",
    "  def __init__(self, code1, code2):\n",
    "    self.code1 = code1\n",
    "    self.code2 = code2\n",
    "    self.max_seq_len = 200\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.code1)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    code1 = self.code1[idx]\n",
    "    code2 = self.code2[idx]\n",
    "\n",
    "    if len(code1) > self.max_seq_len:\n",
    "      code1 = code1[:self.max_seq_len]\n",
    "    elif len(code1) < self.max_seq_len:\n",
    "      code1 = [1] * (self.max_seq_len-len(code1)) + code1\n",
    "\n",
    "    if len(code2) > self.max_seq_len:\n",
    "      code2 = code2[:self.max_seq_len]\n",
    "    elif len(code2) < self.max_seq_len:\n",
    "      code2 = [1] * (self.max_seq_len-len(code2)) + code2\n",
    "\n",
    "    return torch.LongTensor(code1), torch.LongTensor(code2)\n",
    "\n",
    "# 테스트 데이터 로더\n",
    "final_dataset = FIN_Dataset(fidxcode1, fidxcode2)\n",
    "final_loader = DataLoader(final_dataset, batch_size = 32, shuffle=False)\n",
    "\n",
    "class RNN_Text(nn.Module): \n",
    "  def __init__(self, n_vocabs):\n",
    "    super(RNN_Text, self).__init__()\n",
    "\n",
    "    # LookUp Table 정의\n",
    "    # 각 id에 해당하는 token들을 100차원 embedding으로 변환\n",
    "    # 모델과 같이 학습\n",
    "    self.embs = nn.Embedding(n_vocabs, 100)\n",
    "\n",
    "    self.hidden_dim = 32\n",
    "    self.n_layers = 1\n",
    "\n",
    "    # Pytorch에서 제공하는 RNN 모델\n",
    "    self.rnn = nn.RNN(100, self.hidden_dim, self.n_layers, batch_first=True)\n",
    "    \n",
    "    # RNN의 output을 100차원 embedding으로 변환\n",
    "    self.fc = nn.Linear(32, 100)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    x = self.embs(x)\n",
    "\n",
    "    hidden = self.init_hidden(batch_size)\n",
    "\n",
    "    # out: Batch Size X Max Seq Length X Hidden dim\n",
    "    out, hidden = self.rnn(x, hidden) \n",
    "    # out: Batch Size X Hidden dim (시퀀스의 마지막 embedding만 사용(문장 전체의 의미를 가지고 있기 때문))\n",
    "    out = self.fc(out[:,-1])\n",
    "    \n",
    "    return out, hidden\n",
    "  \n",
    "  # hidden_0을 만드는 함수\n",
    "  def init_hidden(self, batch_size):\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "    return hidden\n",
    "\n",
    "print('Starting validation data...')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dict_model = RNN_Text(len(idxdict)*4)\n",
    "dict_model.load_state_dict(torch.load('DictRNN'))\n",
    "dict_model.eval()\n",
    "\n",
    "# rnn_model.eval()\n",
    "\n",
    "# cosine similarity 계산을 위한 객체 생성\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-4)\n",
    "fc = nn.Linear(32, 1)\n",
    "preds = []\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "for batch in tqdm(final_loader):\n",
    "    code1_ids, code2_ids = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs1, _ = dict_model(code1_ids) # code1 임베딩\n",
    "      outputs2, _ = dict_model(code2_ids) # code2 임베딩\n",
    "\n",
    "      pred = cos(outputs1, outputs2)\n",
    "      pred = (pred + 1) / 2\n",
    "\n",
    "      pred[pred > 0.5] = 1.0\n",
    "      pred[pred <= 0.5] = 0.0\n",
    "      pred = pred.numpy()\n",
    "\n",
    "      for icnt in range(len(pred)):\n",
    "          preds.append(int(pred[icnt]))\n",
    "\n",
    "submission['pred'] = preds\n",
    "submission.to_csv('open/testDict.csv', index=False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d7661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
